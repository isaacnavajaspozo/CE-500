# üß† vm-ollama ‚Üí IA / modelos locales
# üß† Ollama se utiliza para modelos LLM localmente (como chat conversacional con IA).
# üêß Lo voy a utilizar para resolver dudas que me aparezcan y en especial comprender la arquitectura de dise√±o de los servicios utilizados en el momento de la instalaci√≥n.
#=========================================================================================================================
[ü¶ô ce-500]::
# Documentaci√≥n oficial   : https://ollama.com/

#=========================================================================================================================
[ü¶ô INSTALACIONES]::
# instalaciones 
apt update
apt upgrade -y

# creo una carpeta en el contenedor e instalo el script
mkdir /ce-500 && cd /ce-500
apt install vim -y
vim script-00.sh
# i - (shift + bot√≥n derecho):
--------------------------------------------------------------------------------------
    # c√≥digo de /SCRIPTS/script-00.sh de este github
--------------------------------------------------------------------------------------

# doy permisos y hago la instalaci√≥n
chmod +x script-00.sh
./script-00.sh
source ~/.bashrc

#=========================================================================================================================
[ü¶ô INSTALACI√ìN OLLAMA]::
# instalo seg√∫n diga Ollama
# https://ollama.com/download/linux
curl -fsSL https://ollama.com/install.sh | sh

# instalo seg√∫n el modelo preferible, en mi caso he elegido Gemma por la variaci√≥n de recursos/calidad y la versi√≥n m√°s reducida (Gemma3 : 1b "815 MB")
# la primera vez dejo que se instalen las dependencias y salgo con ctrl + D
ollama run gemma3:1b 

# creo los alias para agilizar el trabajo
cd
vim ~/.bashrc
----------------------------------------------------
alias _ollama="ollama run gemma3:1b"
alias _borrarCacheOllama="rm -rf ~/.ollama/history"
----------------------------------------------------
SOURCE  ~/.bashrc

# ya funciona pero isntalo un modelo para definir en cada hilo mis prioridades

#=========================================================================================================================
[ü¶ô Creo un modelo (para orientar las respuestas)]::
cd ~/.ollama
mkdir models && cd models
vim Modelfile
----------------------------------------------------
FROM gemma3:1b

SYSTEM "Responde exclusivamente en espa√±ol. Tu estilo es t√©cnico, conciso y directo. Solo responde preguntas relacionadas con programaci√≥n, administraci√≥n de sistemas, hardware electr√≥nico y ciberseguridad. Tu objetivo es ayudar a comprender las arquitecturas y dise√±o de elementos .Rechaza educadamente cualquier pregunta fuera de ese √°mbito."

# Controla aleatoriedad (0.2 = respuestas m√°s precisas y menos creativas)
PARAMETER temperature 0.2
# N√∫mero m√°ximo de tokens generados en la respuesta
PARAMETER num_predict 512
# Limita la elecci√≥n al top 40 tokens m√°s probables (reduce ruido)
PARAMETER top_k 40
# Usa solo tokens con probabilidad acumulada del 90% (top-p sampling)
PARAMETER top_p 0.9
# Penaliza repetici√≥n de tokens ya usados (mejora coherencia)
PARAMETER repeat_penalty 1.1
----------------------------------------------------

    # para ver los par√°metros que puedo configurar
        > ollama show gemma3:1b
    # si quiero agregar nuevos par√°metros y ya est√° creado el modelo tengo que volverlo a generar
        > ollama rm gemma3-es
        > ollama create gemma3-es -f Modelfile


# creo modelo
cd ~/.ollama/models
ollama create gemma3-es -f Modelfile

# utilizo el modelo
ollama run gemma3-es

# sustituyo el alias 
cd
vim ~/.bashrc
----------------------------------------------------
alias _ollama="ollama run gemma3-es"
----------------------------------------------------

#=========================================================================================================================
[ü¶ô Limito la cpu (para no saturar los recursos del pc)]::
## existen varias opciones de hacerlo:

- # esta opci√≥n NO es persistente ante reinicios de servidor:
    # + opcion 1:
    sudo apt update
    sudo apt install cgroup-tools
    
    # Crear un grupo
    cgcreate -g cpu:/ollama_limit
    
    # Limitar a 40% CPU (valor entre 0 y 100000)
    echo 40000 | tee /sys/fs/cgroup/cpu/ollama_limit/cpu.cfs_quota_us
    
    # Ejecutar ollama dentro del grupo
    cgexec -g cpu:ollama_limit ollama run gemma3-es

- # esta opci√≥n SI es persistente ante reinicios de servidor:
    # + opcion 2:
    apt install cpulimit
    cpulimit -l 40 -- ollama run gemma3-es
