#  Ollama se utiliza para modelos LLM localmente (como chat conversacional con IA).
#  Lo voy a utilizar para resolver dudas que me aparezcan y en especial comprender la arquitectura de dise帽o de los servicios utilizados en el momento de la instalaci贸n.
#=========================================================================================================================
[ KIRIBAKO]::
# Documentaci贸n oficial   : https://ollama.com/

#=========================================================================================================================
[ INSTALACIONES]::
# instalaciones 
apt update
apt upgrade -y

# creo una carpeta en el contenedor e instalo el script
mkdir /KIRIBAKO && cd /KIRIBAKO
apt install vim -y
vim script-00.sh
# i - (shift + bot贸n derecho):
--------------------------------------------------------------------------------------
    # c贸digo de /SCRIPTS/script-00.sh de este github
--------------------------------------------------------------------------------------

# doy permisos y hago la instalaci贸n
chmod +x script-00.sh
./script-00.sh
source ~/.bashrc

#=========================================================================================================================
[ INSTALACIN OLLAMA]::
# instalo seg煤n diga Ollama
# https://ollama.com/download/linux
curl -fsSL https://ollama.com/install.sh | sh

# instalo seg煤n el modelo preferible, en mi caso he elegido Gemma por la variaci贸n de recursos/calidad y la versi贸n m谩s reducida (Gemma3 : 1b "815 MB")
# la primera vez dejo que se instalen las dependencias y salgo con ctrl + D
ollama run gemma3:1b 

# creo los alias para agilizar el trabajo
cd
vim ~/.bashrc
----------------------------------------------------
alias _ollama="ollama run gemma3:1b"
alias _borrarCacheOllama="rm -rf ~/.ollama/history"
----------------------------------------------------
SOURCE  ~/.bashrc

# ya funciona pero isntalo un modelo para definir en cada hilo mis prioridades

#=========================================================================================================================
[ Creo un modelo (para orientar las respuestas)]::
cd ~/.ollama
mkdir models && cd models
vim Modelfile
----------------------------------------------------
FROM gemma3:1b

SYSTEM "Responde exclusivamente en espa帽ol. Tu estilo es t茅cnico, conciso y directo. Solo responde preguntas relacionadas con programaci贸n, administraci贸n de sistemas, hardware electr贸nico y ciberseguridad. Tu objetivo es ayudar a comprender las arquitecturas y dise帽o de elementos .Rechaza educadamente cualquier pregunta fuera de ese 谩mbito."

# Controla aleatoriedad (0.2 = respuestas m谩s precisas y menos creativas)
PARAMETER temperature 0.2
# N煤mero m谩ximo de tokens generados en la respuesta
PARAMETER num_predict 512
# Limita la elecci贸n al top 40 tokens m谩s probables (reduce ruido)
PARAMETER top_k 40
# Usa solo tokens con probabilidad acumulada del 90% (top-p sampling)
PARAMETER top_p 0.9
# Penaliza repetici贸n de tokens ya usados (mejora coherencia)
PARAMETER repeat_penalty 1.1
----------------------------------------------------

    # para ver los par谩metros que puedo configurar
        > ollama show gemma3:1b
    # si quiero agregar nuevos par谩metros y ya est谩 creado el modelo tengo que volverlo a generar
        > ollama rm gemma3-es
        > ollama create gemma3-es -f Modelfile


# creo modelo
cd ~/.ollama/models
ollama create gemma3-es -f Modelfile

# utilizo el modelo
ollama run gemma3-es

# sustituyo el alias 
cd
vim ~/.bashrc
----------------------------------------------------
alias _ollama="ollama run gemma3-es"
----------------------------------------------------

#=========================================================================================================================
[ Limito la cpu (para no saturar los recursos del pc)]::
## existen varias opciones de hacerlo:

- # esta opci贸n NO es persistente ante reinicios de servidor:
    # + opcion 1:
    sudo apt update
    sudo apt install cgroup-tools
    
    # Crear un grupo
    cgcreate -g cpu:/ollama_limit
    
    # Limitar a 40% CPU (valor entre 0 y 100000)
    echo 40000 | tee /sys/fs/cgroup/cpu/ollama_limit/cpu.cfs_quota_us
    
    # Ejecutar ollama dentro del grupo
    cgexec -g cpu:ollama_limit ollama run gemma3-es

- # esta opci贸n SI es persistente ante reinicios de servidor:
    # + opcion 2:
    apt install cpulimit
    cpulimit -l 40 -- ollama run gemma3-es
